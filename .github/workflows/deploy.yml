# .github/workflows/deploy.yml
name: CI/CD Pipeline for Employee App (Fargate)

on:
  push:
    branches:
      - main # Trigger on pushes to the main branch
  workflow_dispatch:

env:
  AWS_REGION: ap-south-1 # Your AWS region

jobs:
  ci:
    runs-on: ubuntu-latest
    needs: cd_infrastructure_and_db_init
    permissions:
      id-token: write # Required for OIDC authentication with AWS
      contents: read # Required to checkout code

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9' # Or your application's Python version, matching your Flask app

      - name: Install Python dependencies
        run: pip install -r requirements.txt # Assumes you have a requirements.txt file in your repo root

      #- name: Run unit tests
       # run: pytest # Assumes you use pytest for your tests. Make sure you have test files.

      - name: Configure AWS credentials for ECR login
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.GH_ACTIONS_OIDC_ROLE_NAME }} # IAM Role for GitHub Actions
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

      - name: Build and push Docker image to ECR
        uses: docker/build-push-action@v5
        with:
          context: . # Looks for Dockerfile in the root of your repository
          push: true
          tags: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/employee-app:latest,${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/employee-app:${{ github.sha }}

      # Add Docker image vulnerability scanning here (e.g., Trivy)
      - name: Scan Docker image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/employee-app:${{ github.sha }}
          format: 'table'
          exit-code: '1' # Fail if critical vulnerabilities are found
          ignore-unfixed: true
          severity: 'CRITICAL,HIGH'

  cd_infrastructure_and_db_init:
    runs-on: ubuntu-latest
    # This job does NOT need any other jobs, it can run independently of the image build.
    permissions:
      id-token: write # Required for OIDC authentication with AWS
      contents: read # Required to checkout code for Terraform files

    # Define outputs that can be consumed by subsequent jobs
    outputs:
      ecs_cluster_name: ${{ steps.get-tf-outputs.outputs.ecs_cluster_name }}
      ecs_service_name: ${{ steps.get-tf-outputs.outputs.ecs_service_name }}
      private_subnet_az1_id: ${{ steps.get-tf-outputs.outputs.private_subnet_az1_id }}
      private_subnet_az2_id: ${{ steps.get-tf-outputs.outputs.private_subnet_az2_id }}
      ecs_fargate_sg_id: ${{ steps.get-tf-outputs.outputs.ecs_fargate_sg_id }}
      db_init_task_definition_arn: ${{ steps.get-tf-outputs.outputs.db_init_task_definition_arn }}
      db_init_task_role_arn: ${{ steps.get-tf-outputs.outputs.db_init_task_role_arn }}
      vpc_id: ${{ steps.get-tf-outputs.outputs.vpc_id }}
      # Removed ecr_repository_url output as it's not directly used by the application deployment job anymore

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0 # Specify a compatible Terraform version

      - name: Configure AWS credentials for Terraform and ECS commands
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.GH_ACTIONS_OIDC_ROLE_NAME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init (with reconfigure)
        # Ensure this step fails the job if init doesn't complete successfully
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET_NAME }}" \
            -backend-config="key=employee-app/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_STATE_LOCK_TABLE_NAME }}"
        # No 'sed' commands needed here if your main.tf already has the correct backend names

      - name: Attempt Terraform Plan and Handle Lock
        id: terraform_plan_lock_handler
        run: |
          set +e # Don't exit immediately on error within this script
          echo "Attempting terraform plan to check for state lock..."
          # Pass variables using TF_VAR_ prefix for plan command.
          PLAN_OUTPUT=$(TF_VAR_db_master_username="${{ secrets.DB_MASTER_USERNAME }}" \
                        TF_VAR_my_ip_cidr="${{ secrets.MY_IP_CIDR }}" \
                        TF_VAR_key_pair_name="${{ secrets.KEY_PAIR_NAME }}" \
                        terraform plan 2>&1)
          PLAN_EXIT_CODE=$?

          echo "$PLAN_OUTPUT" # Always print the full output for debugging

          if echo "$PLAN_OUTPUT" | grep -q "Error acquiring the state lock"; then
            echo "State lock detected. Attempting to extract Lock ID..."
            LOCK_ID=$(echo "$PLAN_OUTPUT" | grep "ID:" | awk '{print $2}')

            if [ -n "$LOCK_ID" ]; then
              echo "Extracted Lock ID: $LOCK_ID. Attempting to force-unlock..."
              terraform force-unlock "$LOCK_ID" -force
              if [ $? -eq 0 ]; then
                echo "Successfully force-unlocked the state."
                echo "lock_handled=true" >> "$GITHUB_OUTPUT"
              else
                echo "Failed to force-unlock the state. Manual intervention may be required."
                exit 1
              fi
            else
              echo "State lock error detected, but could not extract Lock ID. Manual intervention may be required."
              exit 1
            fi
          elif [ $PLAN_EXIT_CODE -ne 0 ]; then
            echo "Terraform plan failed for reasons other than a state lock. See output above."
            exit 1
          else
            echo "Terraform plan succeeded, no state lock detected."
            echo "lock_handled=true" >> "$GITHUB_OUTPUT"
          fi
          set -e # Re-enable exit on error

      - name: Terraform Apply (Provision/Update Infrastructure)
        # This step will only run if the previous step successfully handled the lock (or no lock was present)
        if: steps.terraform_plan_lock_handler.outputs.lock_handled == 'true'
        run: |
          echo "Proceeding with terraform apply..."
          terraform apply -auto-approve \
            -var="db_master_username=${{ secrets.DB_MASTER_USERNAME }}" \
            -var="my_ip_cidr=${{ secrets.MY_IP_CIDR }}" \
            -var="key_pair_name=${{ secrets.KEY_PAIR_NAME }}"

      - name: Get Terraform Outputs
        id: get-tf-outputs # ID for this step to retrieve its outputs
        # This step also needs to be conditional on successful lock handling and apply
        if: steps.terraform_plan_lock_handler.outputs.lock_handled == 'true'
        run: |
          echo "ecs_cluster_name=$(terraform output -raw ecs_cluster_name)" >> "$GITHUB_OUTPUT"
          echo "ecs_service_name=$(terraform output -raw ecs_service_name)" >> "$GITHUB_OUTPUT"
          echo "private_subnet_az1_id=$(terraform output -raw private_subnet_az1_id)" >> "$GITHUB_OUTPUT"
          echo "private_subnet_az2_id=$(terraform output -raw private_subnet_az2_id)" >> "$GITHUB_OUTPUT"
          echo "ecs_fargate_sg_id=$(terraform output -raw ecs_fargate_sg_id)" >> "$GITHUB_OUTPUT"
          echo "db_init_task_definition_arn=$(terraform output -raw db_init_task_definition_arn)" >> "$GITHUB_OUTPUT"
          echo "db_init_task_role_arn=$(terraform output -raw db_init_task_role_arn)" >> "$GITHUB_OUTPUT"
          echo "vpc_id=$(terraform output -raw vpc_id)" >> "$GITHUB_OUTPUT"

      - name: Run DB Init Fargate Task
        # This step also needs to be conditional on successful lock handling and apply
        if: steps.terraform_plan_lock_handler.outputs.lock_handled == 'true'
        run: |
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ steps.get-tf-outputs.outputs.ecs_cluster_name }} \
            --task-definition ${{ steps.get-tf-outputs.outputs.db_init_task_definition_arn }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ steps.get-tf-outputs.outputs.private_subnet_az1_id }},${{ steps.get-tf-outputs.outputs.private_subnet_az2_id }}],securityGroups=[${{ steps.get-tf-outputs.outputs.ecs_fargate_sg_id }}],assignPublicIp=DISABLED}" \
            --started-by "github-actions-db-init" \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "Started DB Init Task with ARN: $TASK_ARN"
          aws ecs wait tasks-stopped --cluster ${{ steps.get-tf-outputs.outputs.ecs_cluster_name }} --tasks "$TASK_ARN"
          echo "DB Init Task completed!"

  cd_application_deployment:
    runs-on: ubuntu-latest
    needs: [ci, cd_infrastructure_and_db_init] # This job depends on infrastructure and DB init completing 
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.GH_ACTIONS_OIDC_ROLE_NAME }}
          aws-region: ${{ env.AWS_REGION }}
          

      - name: Debug ECS Cluster and Service Names
        run: |
          echo "DEBUG: Cluster Name: '${{ needs.cd_infrastructure_and_db_init.outputs.ecs_cluster_name }}'"
          echo "DEBUG: Service Name: '${{ needs.cd_infrastructure_and_db_init.outputs.ecs_service_name }}'"

      - name: Update ECS Service with new image
        run: |
          aws ecs update-service \
            --cluster ${{ needs.cd_infrastructure_and_db_init.outputs.ecs_cluster_name }} \
            --service ${{ needs.cd_infrastructure_and_db_init.outputs.ecs_service_name }} \
            --task-definition employee-app-task \
            --force-new-deployment # Forces ECS to launch new tasks with the latest task definition version
